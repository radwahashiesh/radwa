{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.8.16","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"","metadata":{"id":"FOid78DVxWMr","outputId":"8b1d9d70-2b14-44e8-9ea4-26728226b870","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Libraries","metadata":{"id":"43027e21"}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.decomposition import PCA","metadata":{"id":"93069020","execution":{"iopub.status.busy":"2023-06-13T15:54:32.473425Z","iopub.execute_input":"2023-06-13T15:54:32.474091Z","iopub.status.idle":"2023-06-13T15:54:35.514285Z","shell.execute_reply.started":"2023-06-13T15:54:32.474060Z","shell.execute_reply":"2023-06-13T15:54:35.513431Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"#Read dataset ","metadata":{"id":"d9c978e9"}},{"cell_type":"markdown","source":"\n---\nThis code snippet loads data for the  dataset from a specified \nlocation using the load_from_tsfile_to_dataframe function from the sktime package. The training data is stored in the train_x variable as a numpy array of arrays. Similarly, the test data is stored in the test_x variable. The corresponding labels for the training and test data are stored in train_y and test_y variables respectively. The labels are converted to floats using the astype method. This code can be used as a starting point for any analysis or modeling tasks involving the Cricket dataset.\n\n\n---\n\n","metadata":{"id":"PNTrsMxJFoXt"}},{"cell_type":"code","source":"def read_data_and_labels(train_filename, test_filename):\n    train_x = []\n    train_y = []\n    test_x = []\n    test_y = []\n\n    # Read training data and labels\n    with open(train_filename, 'r') as train_file:\n        train_reader = csv.reader(train_file, delimiter=',')\n        for row in train_reader:\n            if row and not row[0].startswith(('#', '@')):\n                label = row[-1].split(':')[-1]  # Extract label from the value after colon\n                train_x.append([float(x.split(':')[0]) for x in row[:-1]])  # Remove colon and convert elements to float\n                train_y.append(label)\n\n    # Read testing data and labels\n    with open(test_filename, 'r') as test_file:\n        test_reader = csv.reader(test_file, delimiter=',')\n        for row in test_reader:\n            if row and not row[0].startswith(('#', '@')):\n                label = row[-1].split(':')[-1]  # Extract label from the value after colon\n                test_x.append([float(x.split(':')[0]) for x in row[:-1]])  # Remove colon and convert elements to float\n                test_y.append(label)\n\n    return train_x, train_y, test_x, test_y\n","metadata":{"id":"a4f364e7","execution":{"iopub.status.busy":"2023-06-13T15:54:35.515784Z","iopub.execute_input":"2023-06-13T15:54:35.516248Z","iopub.status.idle":"2023-06-13T15:54:35.527618Z","shell.execute_reply.started":"2023-06-13T15:54:35.516221Z","shell.execute_reply":"2023-06-13T15:54:35.526694Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import csv\n#@Usage:\ntrain_filename = '/kaggle/input/multivariate2018-python/Multivariate_ts/InsectWingbeat/InsectWingbeat_TRAIN.ts'\ntest_filename = '/kaggle/input/multivariate2018-python/Multivariate_ts/InsectWingbeat/InsectWingbeat_TEST.ts'\ntrain_x, train_y, test_x, test_y = read_data_and_labels(train_filename, test_filename)","metadata":{"id":"99a60cd8","execution":{"iopub.status.busy":"2023-06-13T15:54:35.528717Z","iopub.execute_input":"2023-06-13T15:54:35.529339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"c1dfrzCmtfwF","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_y = np.array(train_y)\ntrain_x = np.array(train_x)\ntest_y = np.array(test_y)\ntest_x = np.array(test_x)","metadata":{"id":"C2dEPOkTtfwG","outputId":"4fca721d-4dff-486b-d2ac-58ff9ddc1567","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# apply pca ","metadata":{"id":"d0265919"}},{"cell_type":"markdown","source":"The above code represents a data preprocessing pipeline for the cricket dataset.\n\n\n---\n\n\nThe preprocess_data() function takes in train_x and test_x as inputs, which are the training and testing feature sets respectively. These feature sets are first standardized using the StandardScaler() function from scikit-learn's preprocessing module. Standardization involves centering the data and scaling it to unit variance. This is done to ensure that all features are on the same scale, which is important for many machine learning algorithms.\n\n\n---\n\n\nNext, Principal Component Analysis (PCA) is applied to the standardized data using the PCA() function from scikit-learn's decomposition module. PCA is a dimensionality reduction technique that transforms the original feature space into a new space of lower dimensionality. This is done by identifying the principal components, which are linear combinations of the original features that explain the most variance in the data.\n\n---\n\nThe n_components parameter specifies the number of principal components to keep. In this case, it is set to 5. Finally, the preprocessed training and testing feature sets are returned.\n\n---\n\nOverall, this data preprocessing pipeline can help improve the performance of machine learning models on the cricket dataset by reducing the dimensionality of the feature space and scaling the data to a common range.","metadata":{"id":"HI6wOdxKFw0b"}},{"cell_type":"code","source":"","metadata":{"id":"fb6aeef3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\ndef preprocess_data(train_x, test_x, n_components=5):\n    # Convert train_x and test_x to 2D arrays with padding\n    train_x_padded = np.array([x + [0] * (max_len - len(x)) for x in train_x])\n    test_x_padded = np.array([x + [0] * (max_len - len(x)) for x in test_x])\n\n    # Convert train_x and test_x to DataFrames\n    train_x_df = pd.DataFrame(train_x_padded)\n    test_x_df = pd.DataFrame(test_x_padded)\n\n    # Scale the data\n    scaler = StandardScaler()\n    train_x_scaled = scaler.fit_transform(train_x_df)\n    test_x_scaled = scaler.transform(test_x_df)\n\n    # Apply PCA\n    pca = PCA(n_components=n_components)\n    train_x_pca = pca.fit_transform(train_x_scaled)\n    test_x_pca = pca.transform(test_x_scaled)\n\n    return train_x_pca, test_x_pca\n\n# Determine the maximum length of sequences in train_x and test_x\nmax_len = max(max(len(x) for x in train_x), max(len(x) for x in test_x))\n\ntrain_x, test_x = preprocess_data(train_x, test_x, n_components=5)","metadata":{"id":"Idp3lzvWtfwL","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"x9CLPzdAtfwN","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_y)\nprint(test_y)\n\nfrom sklearn.preprocessing import LabelEncoder\n\n# Create LabelEncoder object\nlabel_encoder = LabelEncoder()\n\n# Fit label encoder and transform train_y\ntrain_y = label_encoder.fit_transform(train_y)\n# Fit label encoder and transform train_y\ntest_y = label_encoder.fit_transform(test_y)\n\n\nprint(train_y)\nprint(test_y)\n\n\n# Convert labels to one-hot encoding\nfrom keras.utils import to_categorical\ntrain_y = np.array(train_y)\ntest_y = np.array(test_y)\ntrain_y = train_y.astype(int)\ntest_y = test_y.astype(int)","metadata":{"id":"705397c8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The code snippet you provided shows how to convert integer labels to one-hot encoded labels using the to_categorical function from Keras.\n\n\n---\n\n\nOne-hot encoding is a technique used to represent categorical variables as binary vectors. In the context of classification tasks, this is often used to represent the target variable, where each category is assigned a unique binary vector. Each binary vector has a length equal to the total number of categories, and a value of 1 is assigned to the index corresponding to the category, while all other values are 0.","metadata":{"id":"Q_7jeZXYF8PD"}},{"cell_type":"code","source":"\n# Convert labels to one-hot encoding\nfrom keras.utils import to_categorical\ntrain_y = np.array(train_y)\ntest_y = np.array(test_y)\ntrain_y = train_y.astype(int)\ntest_y = test_y.astype(int)\ntrain_y = to_categorical(train_y)\ntest_y = to_categorical(test_y)","metadata":{"id":"ec22ebee","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_y.shape)\nprint(train_x.shape)\n\nprint(test_y.shape)\nprint(test_x.shape)\n\n","metadata":{"id":"18jDXqv_tfwQ","outputId":"b2b42798-81b1-494e-d347-429447feff6b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n---\nThis line of code calculates the number of classes in the dataset. It uses \nthe shape attribute of the numpy array train_y to get the number of rows (which represents the number of instances in the dataset) and the number of columns (which represents the number of classes). Since the dataset uses one-hot encoding for the labels, each row of train_y contains only one non-zero value, which corresponds to the class label of that instance. Therefore, the number of columns in train_y is equal to the number of classes in the dataset. The variable num_classes is assigned this value, which will be used later in the code for defining the output layer of the neural network model.\n\n\n---\n\n\n","metadata":{"id":"VotbF7w_GG_O"}},{"cell_type":"code","source":"num_classes = (train_y.shape[1])","metadata":{"id":"1ba1aba3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Resnet ","metadata":{"id":"2293a53b"}},{"cell_type":"markdown","source":"This is a function that defines a ResNet model using Keras. It takes two arguments: input_shape, which specifies the shape of the input data, and num_classes, which specifies the number of classes to predict. The model has four convolutional blocks, each consisting of a convolutional layer, batch normalization, ReLU activation, and dropout. The first three blocks have 64, 128, and 256 filters, respectively, while the fourth block has 512 filters. A shortcut connection is added from the input layer to the output of the fourth block, and the output is then passed through a final ReLU activation and flattened before being passed through a fully connected softmax layer with num_classes outputs. The model is compiled with the Adam optimizer, categorical crossentropy loss, and accuracy metric. This ResNet implementation is a modified version of the original ResNet architecture proposed by He et al. (2015) and is commonly used for time series classification tasks.","metadata":{"id":"_x9sXH0JGT65"}},{"cell_type":"code","source":"from keras.initializers import *\n\ndef resnet(input_shape, num_classes, initializer):\n    # Define the input layer\n    input_layer = Input(shape=input_shape)\n\n    # First convolutional block\n    conv1 = Conv1D(filters=64, kernel_size=3, padding='same', kernel_initializer=initializer)(input_layer)\n    norm1 = BatchNormalization()(conv1)\n    relu1 = Activation('relu')(norm1)\n    drop1 = Dropout(0.2)(relu1)\n\n    # Second convolutional block\n    conv2 = Conv1D(filters=128, kernel_size=3, padding='same', kernel_initializer=initializer)(drop1)\n    norm2 = BatchNormalization()(conv2)\n    relu2 = Activation('relu')(norm2)\n    drop2 = Dropout(0.2)(relu2)\n\n    # Third convolutional block\n    conv3 = Conv1D(filters=256, kernel_size=3, padding='same', kernel_initializer=initializer)(drop2)\n    norm3 = BatchNormalization()(conv3)\n    relu3 = Activation('relu')(norm3)\n    drop3 = Dropout(0.2)(relu3)\n\n    # Fourth convolutional block\n    conv4 = Conv1D(filters=512, kernel_size=3, padding='same', kernel_initializer=initializer)(drop3)\n    norm4 = BatchNormalization()(conv4)\n    relu4 = Activation('relu')(norm4)\n    drop4 = Dropout(0.2)(relu4)\n\n    # Add shortcut connection\n    shortcut = Conv1D(filters=512, kernel_size=1, padding='same', kernel_initializer=initializer)(input_layer)\n    shortcut = BatchNormalization()(shortcut)\n    output = Add()([drop4, shortcut])\n    output = Activation('relu')(output)\n\n    # Flatten and output\n    flatten = Flatten()(output)\n    dense = Dense(num_classes, activation='softmax')(flatten)\n\n    # Create and compile the model\n    model = Model(inputs=input_layer, outputs=dense)\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n    return model\n","metadata":{"id":"ff1f98b1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom keras.models import Model\nfrom keras.layers import Input, Conv1D, BatchNormalization, Activation, Add, Flatten, Dense, Dropout\nfrom keras.regularizers import l2\ninitializers = [GlorotUniform(), HeUniform(), LecunUniform(), RandomUniform(), Zeros()]\n\n# Create subplots for accuracy and loss\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 10))\n\nfor initializer in initializers:\n    # Reshape the input data\n    train_x = np.reshape(train_x, (train_x.shape[0], train_x.shape[1], 1))\n    test_x = np.reshape(test_x, (test_x.shape[0], test_x.shape[1], 1))\n\n    # Define and train the model with the current initializer\n    model = resnet(input_shape=(train_x.shape[1],1), num_classes=num_classes, initializer=initializer)\n    history = model.fit(train_x, train_y, validation_data=(test_x, test_y), epochs=100, batch_size=32)\n    loss, accuracy = model.evaluate(test_x, test_y)\n    print(f\"Initializer: {str(initializer)}\")\n    print(f\"Test loss: {loss}\")\n    print(f\"Test accuracy: {accuracy}\")\n    print()\n\n    # Plot accuracy graph\n    ax1.plot(history.history['accuracy'], label=str(initializer))\n    ax1.plot(history.history['val_accuracy'], label=str(initializer) + ' (val)')\n    ax1.set_title('Model accuracy')\n    ax1.set_xlabel('Epoch')\n    ax1.set_ylabel('Accuracy')\n    ax1.legend(loc='lower right')\n\n    # Plot loss graph\n    ax2.plot(history.history['loss'], label=str(initializer))\n    ax2.plot(history.history['val_loss'], label=str(initializer) + ' (val)')\n    ax2.set_title('Model loss')\n    ax2.set_xlabel('Epoch')\n    ax2.set_ylabel('Loss')\n    ax2.legend(loc='upper right')\n\n# Adjust spacing between subplots\nplt.tight_layout()\n\n# Show the plot\nplt.show()\n","metadata":{"id":"71b9610e","outputId":"90b620ae-0741-496c-ca61-7853a4260beb","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"bbe9cbb5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"66b22bcb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Tapent","metadata":{"id":"37cca367"}},{"cell_type":"markdown","source":"The tapnet function defines a neural network model using convolutional layers for feature extraction and a dense layer for classification. The model has three convolutional blocks, each consisting of a 1D convolutional layer with ReLU activation followed by a max pooling layer. The output of the last convolutional block is flattened and fed into a dense layer with softmax activation, which produces the output probabilities for each class. The model is compiled with Adam optimizer and categorical cross-entropy loss function. This architecture is often used for audio processing tasks such as speech recognition and environmental sound classification.","metadata":{"id":"n8oDj_gFGa6a"}},{"cell_type":"code","source":"from keras.models import Model\nfrom keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, concatenate, Dropout\nfrom keras.initializers import GlorotUniform, HeUniform, LecunUniform, RandomUniform, Zeros\nimport matplotlib.pyplot as plt\n\ndef tapnet(input_shape, num_classes, initializer):\n    # Define the input layer\n    input_layer = Input(shape=input_shape)\n\n    # First convolutional block\n    conv1 = Conv1D(filters=64, kernel_size=3, activation='relu', padding='same', kernel_initializer=initializer)(input_layer)\n    pool1 = MaxPooling1D(pool_size=2)(conv1)\n\n    # Second convolutional block\n    conv2 = Conv1D(filters=128, kernel_size=3, activation='relu', padding='same', kernel_initializer=initializer)(pool1)\n    pool2 = MaxPooling1D(pool_size=2)(conv2)\n\n    # Third convolutional block\n    conv3 = Conv1D(filters=256, kernel_size=3, activation='relu', padding='same', kernel_initializer=initializer)(pool2)\n\n    # Flatten the output from the convolutional layers\n    flatten = Flatten()(conv3)\n\n    # Define the output layer\n    output_layer = Dense(units=num_classes, activation='softmax')(flatten)\n\n    # Create the model\n    model = Model(inputs=input_layer, outputs=output_layer)\n\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n    return model\n","metadata":{"id":"68bf9177","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"initializers = [GlorotUniform(), HeUniform(), LecunUniform(), RandomUniform(), Zeros()]\n\n# Create subplots for accuracy and loss\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 10))\n\nfor initializer in initializers:\n    # Reshape the input data\n    train_x = np.reshape(train_x, (train_x.shape[0], train_x.shape[1], 1))\n    test_x = np.reshape(test_x, (test_x.shape[0], test_x.shape[1], 1))\n\n    # Define and train the model with the current initializer\n    model = tapnet(input_shape=(train_x.shape[1],1), num_classes=num_classes, initializer=initializer)\n    history = model.fit(train_x, train_y, validation_data=(test_x, test_y), epochs=100, batch_size=6)\n    loss, accuracy = model.evaluate(test_x, test_y)\n    print(f\"Initializer: {str(initializer)}\")\n    print(f\"Test loss: {loss}\")\n    print(f\"Test accuracy: {accuracy*100}\")\n    print()\n\n    # Plot accuracy graph\n    ax1.plot(history.history['accuracy'], label=str(initializer))\n    ax1.plot(history.history['val_accuracy'], label=str(initializer) + ' (val)')\n    ax1.set_title('Model accuracy')\n    ax1.set_xlabel('Epoch')\n    ax1.set_ylabel('Accuracy')\n    ax1.legend(loc='lower right')\n\n    # Plot loss graph\n    ax2.plot(history.history['loss'], label=str(initializer))\n    ax2.plot(history.history['val_loss'], label=str(initializer) + ' (val)')\n    ax2.set_title('Model loss')\n    ax2.set_xlabel('Epoch')\n    ax2.set_ylabel('Loss')\n    ax2.legend(loc='upper right')\n\n# Adjust spacing between subplots\nplt.tight_layout()\n\n# Show the plot\nplt.show()","metadata":{"id":"5b2096fa","outputId":"6d22c220-d6a8-497c-bf66-2decd9f85e70","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"Yp5oMr231ww2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CIF","metadata":{"id":"7j6Hg4GE277_"}},{"cell_type":"markdown","source":"the model architecture in the code you provided is a variant of the CIF (Convolutional-Inception-FullyConnected) model, which combines convolutional and fully connected layers to achieve high performance in image classification tasks.\n\n\n---\n\nThis is a Convolutional Neural Network (CNN) model that has three convolutional blocks, followed by a GlobalAveragePooling1D layer and a dense output layer with a sigmoid activation function. It has been trained using the Adam optimizer and categorical cross-entropy loss function, and evaluated on a test set using accuracy as the evaluation metric.\n\nThe model architecture is suitable for a 1D Convolutional model to classify time-series data. The input shape is defined as (5, 1), meaning that the model takes in sequences of length 5 with a single feature dimension. The first convolutional block has 32 filters with a kernel size of 3, followed by another identical convolutional layer and a 25% dropout rate. The second convolutional block has 64 filters with a kernel size of 3, followed by another identical convolutional layer and a 25% dropout rate. The third convolutional block has 128 filters with a kernel size of 3, followed by another identical convolutional layer and a 25% dropout rate. The output of the third convolutional block is then passed through a GlobalAveragePooling1D layer to produce a fixed-size output. Finally, a dense output layer with a sigmoid activation function is used to produce the classification output.\n","metadata":{"id":"942d9093"}},{"cell_type":"code","source":"from keras.layers import Input, Conv1D, MaxPooling1D, GlobalAveragePooling1D, Dense\n\ndef CIF(input_shape, num_classes, initializer):\n    # Define the input layer\n    input_layer = Input(shape=input_shape)\n\n    # First convolutional block\n    conv1 = Conv1D(filters=64, kernel_size=3, activation='relu', padding='same', kernel_initializer=initializer)(input_layer)\n    pool1 = MaxPooling1D(pool_size=2)(conv1)\n\n    # Second convolutional block\n    conv2 = Conv1D(filters=128, kernel_size=3, activation='relu', padding='same', kernel_initializer=initializer)(pool1)\n    pool2 = MaxPooling1D(pool_size=2)(conv2)\n\n    # Third convolutional block\n    conv3 = Conv1D(filters=256, kernel_size=3, activation='relu', padding='same', kernel_initializer=initializer)(pool2)\n\n    # Flatten the output from the convolutional layers\n    flatten = Flatten()(conv3)\n\n    # Define the output layer\n    output_layer = Dense(units=num_classes, activation='softmax')(flatten)\n\n    # Create the model\n    model = Model(inputs=input_layer, outputs=output_layer)\n\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n    return model\n\n","metadata":{"id":"251e7ea3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ninitializers = [GlorotUniform(), HeUniform(), LecunUniform(), RandomUniform(), Zeros()]\n\n# Create subplots for accuracy and loss\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 10))\n\nfor initializer in initializers:\n    # Reshape the input data\n    train_x_reshaped = np.reshape(train_x, (train_x.shape[0], train_x.shape[1], 1))\n    test_x_reshaped = np.reshape(test_x, (test_x.shape[0], test_x.shape[1], 1))\n\n    # Define and train the model with the current initializer\n    model = CIF(input_shape=(train_x_reshaped.shape[1], train_x_reshaped.shape[2]), num_classes=num_classes, initializer=initializer)\n    history = model.fit(train_x_reshaped, train_y, validation_data=(test_x_reshaped, test_y), epochs=100, batch_size=6)\n    loss, accuracy = model.evaluate(test_x_reshaped, test_y)\n    print(f\"Initializer: {str(initializer)}\")\n    print(f\"Test loss: {loss}\")\n    print(f\"Test accuracy: {accuracy*100}\")\n    print()\n\n    # Plot accuracy graph\n    ax1.plot(history.history['accuracy'], label=str(initializer))\n    ax1.plot(history.history['val_accuracy'], label=str(initializer) + ' (val)')\n    ax1.set_title('Model accuracy')\n    ax1.set_xlabel('Epoch')\n    ax1.set_ylabel('Accuracy')\n    ax1.legend(loc='lower right')\n\n    # Plot loss graph\n    ax2.plot(history.history['loss'], label=str(initializer))\n    ax2.plot(history.history['val_loss'], label=str(initializer) + ' (val)')\n    ax2.set_title('Model loss')\n    ax2.set_xlabel('Epoch')\n    ax2.set_ylabel('Loss')\n    ax2.legend(loc='upper right')\n\n# Adjust spacing between subplots\nplt.tight_layout()\n\n# Show the plot\nplt.show()","metadata":{"id":"aa847e7d","outputId":"642fc303-6dae-458a-be5d-ad5a60bcd0be","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# INCEPTION","metadata":{"id":"5cPYue9-56oO"}},{"cell_type":"markdown","source":"This code defines an Inception model for 1D convolutional neural networks in Keras. The model architecture is based on the Inception module, which uses multiple convolutional filters with different kernel sizes and max pooling to capture different levels of abstraction in the input data.\n\n\n---\n\n\nThe inception_module function defines a single Inception module, which takes an input tensor x and applies four different convolutional filters with different kernel sizes (1, 3, and 5) and max pooling. The outputs of these filters are then concatenated along the feature dimension, resulting in a tensor with higher dimensionality.\n\n\n---\n\n\nThe create_inception_model function uses the inception_module function to build a deep Inception model. The input shape and number of output classes are specified as arguments. The function builds a series of Inception modules with increasing filter sizes, followed by a global average pooling layer and a dense output layer with softmax activation. The model is compiled with categorical cross-entropy loss and the Adam optimizer.","metadata":{"id":"Q8cJT7jbKfRt"}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras.initializers import GlorotUniform, HeUniform, LecunUniform, RandomUniform, Zeros\n\ndef inception_module(x, filter_size, initializer):\n    conv1 = layers.Conv1D(filter_size, kernel_size=1, padding='same', activation='relu', kernel_initializer=initializer)(x)\n    conv3 = layers.Conv1D(filter_size, kernel_size=3, padding='same', activation='relu', kernel_initializer=initializer)(x)\n    conv5 = layers.Conv1D(filter_size, kernel_size=5, padding='same', activation='relu', kernel_initializer=initializer)(x)\n    max_pool = layers.MaxPooling1D(pool_size=3, strides=1, padding='same')(x)\n    concat = layers.concatenate([conv1, conv3, conv5, max_pool], axis=-1)\n    return concat\n\ndef create_inception_model(input_shape, num_classes, initializer):\n    inputs = layers.Input(shape=input_shape)\n    x = inputs\n    x = inception_module(x, 32, initializer)\n    x = inception_module(x, 64, initializer)\n    x = inception_module(x, 128, initializer)\n    x = inception_module(x, 256, initializer)\n    x = layers.Flatten()(x)\n    outputs = layers.Dense(num_classes, activation='softmax')(x)\n    model = models.Model(inputs=inputs, outputs=outputs)\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model","metadata":{"id":"d6411b89","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Define the initializers\ninitializers = [GlorotUniform(), HeUniform(), LecunUniform(), RandomUniform(), Zeros()]\n\n# Reshape the input data\ntrain_x = np.reshape(train_x, (train_x.shape[0], train_x.shape[1], 1))\ntest_x = np.reshape(test_x, (test_x.shape[0], test_x.shape[1], 1))\n\n# Create subplots for accuracy and loss\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 10))\n\nfor initializer in initializers:\n    # Create the Inception-like model with the current initializer\n    model = create_inception_model(input_shape=train_x.shape[1:], num_classes=num_classes, initializer=initializer)\n\n    # Train the model for 100 epochs\n    history = model.fit(train_x, train_y, validation_data=(test_x, test_y), epochs=100, batch_size=32)\n\n    # Evaluate the model on the test set\n    loss, accuracy = model.evaluate(test_x, test_y)\n    print(f\"Initializer: {str(initializer)}\")\n    print(f\"Test loss: {loss}\")\n    print(f\"Test accuracy: {accuracy*100}\")\n    print()\n\n    # Plot accuracy graph\n    ax1.plot(history.history['accuracy'], label=str(initializer))\n    ax1.plot(history.history['val_accuracy'], label=str(initializer) + ' (val)')\n    ax1.set_title('Model accuracy')\n    ax1.set_xlabel('Epoch')\n    ax1.set_ylabel('Accuracy')\n    ax1.legend(loc='lower right')\n\n    # Plot loss graph\n    ax2.plot(history.history['loss'], label=str(initializer))\n    ax2.plot(history.history['val_loss'], label=str(initializer) + ' (val)')\n    ax2.set_title('Model loss')\n    ax2.set_xlabel('Epoch')\n    ax2.set_ylabel('Loss')\n    ax2.legend(loc='upper right')\n\n# Adjust spacing between subplots\nplt.tight_layout()\n\n# Show the plot\nplt.show()","metadata":{"id":"84d438d4","outputId":"7b0c590f-2be5-4726-bf73-10d5781ccb69","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Rocket ","metadata":{"id":"PKGJAbbTq58Y"}},{"cell_type":"markdown","source":"The Rocket model is a simple yet effective 1D Convolutional Neural Network (CNN) architecture designed for time series classification tasks. The model is based on the idea of Random Convolutional Kernels (RCKs), which are small filters randomly generated from the input data to capture local patterns.\n\nThe Rocket model consists of five convolutional blocks, each with a different number of filters and kernel size, and a final dense layer for classification. The convolutional blocks are designed to learn increasingly complex representations of the input data, while the dropout layers help prevent overfitting.\n\nThe model uses batch normalization to normalize the activations between layers and to speed up the training process. Additionally, the model applies L2 regularization to the convolutional layers to prevent overfitting.\n\nOverall, the Rocket model is a simple, yet effective architecture for time series classification tasks, and it has been shown to achieve state-of-the-art performance on several benchmark datasets. Its simplicity and efficiency make it a good candidate for real-world applications where computational resources are limited.","metadata":{"id":"biCpsUMFq58Y"}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom keras.models import Model\nfrom keras.layers import Input, Conv1D, BatchNormalization, Activation, Add, Flatten, Dense, Dropout\nfrom keras.regularizers import l2\nfrom keras.initializers import GlorotUniform, HeUniform, LecunUniform, RandomUniform, Zeros\n\ndef rocket(input_shape, num_classes, initializer):\n    # Define the input layer\n    input_layer = Input(shape=input_shape)\n\n    # First convolutional block\n    conv1 = Conv1D(filters=16, kernel_size=11, padding='same', kernel_regularizer=l2(1e-4), kernel_initializer=initializer)(input_layer)\n    norm1 = BatchNormalization()(conv1)\n    relu1 = Activation('relu')(norm1)\n    drop1 = Dropout(0.2)(relu1)\n\n    # Second convolutional block\n    conv2 = Conv1D(filters=32, kernel_size=9, padding='same', kernel_regularizer=l2(1e-4), kernel_initializer=initializer)(drop1)\n    norm2 = BatchNormalization()(conv2)\n    relu2 = Activation('relu')(norm2)\n    drop2 = Dropout(0.2)(relu2)\n\n    # Third convolutional block\n    conv3 = Conv1D(filters=64, kernel_size=7, padding='same', kernel_regularizer=l2(1e-4), kernel_initializer=initializer)(drop2)\n    norm3 = BatchNormalization()(conv3)\n    relu3 = Activation('relu')(norm3)\n    drop3 = Dropout(0.2)(relu3)\n\n    # Fourth convolutional block\n    conv4 = Conv1D(filters=128, kernel_size=5, padding='same', kernel_regularizer=l2(1e-4), kernel_initializer=initializer)(drop3)\n    norm4 = BatchNormalization()(conv4)\n    relu4 = Activation('relu')(norm4)\n    drop4 = Dropout(0.2)(relu4)\n\n    # Fifth convolutional block\n    conv5 = Conv1D(filters=256, kernel_size=3, padding='same', kernel_regularizer=l2(1e-4), kernel_initializer=initializer)(drop4)\n    norm5 = BatchNormalization()(conv5)\n    relu5 = Activation('relu')(norm5)\n    drop5 = Dropout(0.2)(relu5)\n\n    # Flatten and output\n    flatten = Flatten()(drop5)\n    dense = Dense(num_classes, activation='softmax')(flatten)\n\n    # Create and compile the model\n    model = Model(inputs=input_layer, outputs=dense)\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n    return model\n","metadata":{"id":"UPRxDSH2q58Z","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Define the initializers\ninitializers = [GlorotUniform(), HeUniform(), LecunUniform(), RandomUniform(), Zeros()]\n\n# Reshape the input data\ntrain_x = np.reshape(train_x, (train_x.shape[0], train_x.shape[1], 1))\ntest_x = np.reshape(test_x, (test_x.shape[0], test_x.shape[1], 1))\n\n# Create subplots for accuracy and loss\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 10))\n\nfor initializer in initializers:\n    # Define and train the model with the current initializer\n    model = rocket(input_shape=(train_x.shape[1], 1), num_classes=num_classes, initializer=initializer)\n    history = model.fit(train_x, train_y, validation_data=(test_x, test_y), epochs=100, batch_size=32)\n    loss, accuracy = model.evaluate(test_x, test_y)\n    print(f\"Initializer: {str(initializer)}\")\n    print(f\"Test loss: {loss}\")\n    print(f\"Test accuracy: {accuracy*100}\")\n    print()\n\n    # Plot accuracy graph\n    ax1.plot(history.history['accuracy'], label=str(initializer))\n    ax1.plot(history.history['val_accuracy'], label=str(initializer) + ' (val)')\n    ax1.set_title('Model accuracy')\n    ax1.set_xlabel('Epoch')\n    ax1.set_ylabel('Accuracy')\n    ax1.legend(loc='lower right')\n\n    # Plot loss graph\n    ax2.plot(history.history['loss'], label=str(initializer))\n    ax2.plot(history.history['val_loss'], label=str(initializer) + ' (val)')\n    ax2.set_title('Model loss')\n    ax2.set_xlabel('Epoch')\n    ax2.set_ylabel('Loss')\n    ax2.legend(loc='upper right')\n\n# Adjust spacing between subplots\nplt.tight_layout()\n\n# Show the plot\nplt.show()\n","metadata":{"id":"0UP8qcLxq58Z","outputId":"457fb217-4884-4aa6-e10c-1a8bb12ea001","trusted":true},"execution_count":null,"outputs":[]}]}